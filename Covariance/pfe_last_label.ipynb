{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import moabb\n",
    "import numpy as np\n",
    "from moabb.datasets import BNCI2014_001\n",
    "dataset = BNCI2014_001()\n",
    "sessions = dataset.get_data(subjects=[1])\n",
    "subject = 1\n",
    "session_name = \"0train\"\n",
    "run_name = \"0\"\n",
    "raw = sessions[subject][session_name][run_name]\n",
    "raw=raw.pick_types(eeg=True,exclude='bads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def segment_labels(m, l, s, raw):\n",
    "    n = (m - l) // s + 1\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        start = i * s\n",
    "        end = start + l\n",
    "        # Convertir les indices de segment en temps\n",
    "        start_time = start / raw.info['sfreq']\n",
    "        end_time = end / raw.info['sfreq']\n",
    "        # Trouver les annotations qui chevauchent ce segment\n",
    "        segment_annotations = raw.annotations[\n",
    "            (raw.annotations.onset <= end_time) & \n",
    "            ((raw.annotations.onset + raw.annotations.duration) >= start_time)\n",
    "        ]\n",
    "        segment_labels = segment_annotations.description.tolist()\n",
    "        labels.append(segment_labels)\n",
    "    \n",
    "    return labels\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "m = raw.n_times  # nombre total d'échantillons\n",
    "l = 500  # longueur du segment en échantillons (par exemple 2 secondes si sfreq=250 Hz)\n",
    "s = 250  # stride en échantillons (par exemple 1 seconde si sfreq=250 Hz)\n",
    "\n",
    "labels = segment_labels(m, l, s, raw)\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmentation:\n",
    "    def __init__(self, raw, l, s):\n",
    "        \"\"\"\n",
    "        Initialiser avec les données X, la longueur des segments l, et le décalage s.\n",
    "        X : numpy array où les lignes sont les variables et les colonnes sont les échantillons.\n",
    "        \"\"\"\n",
    "        self.raw = raw\n",
    "        self.l = l\n",
    "        self.s = s\n",
    "        self.m = raw.get_data().shape[1]\n",
    "\n",
    "    def segment_labels(self):\n",
    "        n = (self.m - self.l) // self.s + 1\n",
    "        labels = []\n",
    "\n",
    "        for i in range(0,n):\n",
    "            start = i * self.s\n",
    "            end = start + self.l\n",
    "            # Convertir les indices de segment en temps\n",
    "            start_time = start / self.raw.info['sfreq']\n",
    "            end_time = end / self.raw.info['sfreq']\n",
    "            # Trouver les annotations qui chevauchent ce segment\n",
    "            segment_annotations = self.raw.annotations[\n",
    "                (self.raw.annotations.onset <= end_time) & \n",
    "                ((self.raw.annotations.onset + self.raw.annotations.duration) >= start_time)\n",
    "            ]\n",
    "            segment_labels = segment_annotations.description.tolist()\n",
    "            labels.append(segment_labels)\n",
    "\n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CovarianceCalculator:\n",
    "    def __init__(self, X, l, s):\n",
    "        \"\"\"\n",
    "        Initialiser avec les données X, la longueur des segments l, et le décalage s.\n",
    "        X : numpy array où les lignes sont les variables et les colonnes sont les échantillons.\n",
    "        \"\"\"\n",
    "        self.X = X\n",
    "        self.l = l\n",
    "        self.s = s\n",
    "        self.C = []\n",
    "        self.calculate_covariance_matrices()\n",
    "    \n",
    "    def calculate_covariance_matrices(self):\n",
    "        \"\"\"\n",
    "        Calculer les matrices de covariance successives à partir des données initiales.\n",
    "        \"\"\"\n",
    "        m = self.X.shape[1]  # nombre de colonnes (échantillons)\n",
    "        n = (m - self.l) // self.s + 1\n",
    "        \n",
    "        for k in range(n):\n",
    "            start_idx = k * self.s\n",
    "            end_idx = start_idx + self.l\n",
    "            segment = self.X[:, start_idx:end_idx]\n",
    "            cov_matrix = np.cov(segment)\n",
    "            self.C.append(cov_matrix)\n",
    "    \n",
    "    \n",
    "        \n",
    "        return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import sqrtm, logm\n",
    "\n",
    "class Transformation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def project_to_tangent(self, cov_matrix_current, cov_matrix_next):\n",
    "        sqrt_B = sqrtm(cov_matrix_next)\n",
    "        sqrt_inv_B = np.linalg.inv(sqrt_B)\n",
    "        transformed_A = np.dot(np.dot(sqrt_inv_B, cov_matrix_current), sqrt_inv_B)\n",
    "        log_transformed_A = logm(transformed_A)\n",
    "        return log_transformed_A\n",
    "\n",
    "    def transport_to_tangent(self, delta_matrix, tangent_plane_start, tangent_plane_end):\n",
    "        inv_B = np.linalg.inv(tangent_plane_end)\n",
    "        A_invB = np.dot(tangent_plane_start, inv_B)\n",
    "        E = sqrtm(A_invB)\n",
    "        transported_Delta = np.dot(np.dot(E, delta_matrix), E.T)\n",
    "        return transported_Delta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trajectory:\n",
    "    def __init__(self, covariance_matrices):\n",
    "        \"\"\"\n",
    "        Initialiser avec une liste de matrices de covariance.\n",
    "        \"\"\"\n",
    "        self.C = covariance_matrices\n",
    "        self.transformation = Transformation()\n",
    "        self.Deltas = self.calculate_deltas()\n",
    "\n",
    "    def calculate_deltas(self):\n",
    "        \"\"\"\n",
    "        Calculer les incréments et les transports successifs pour reconstruire la trajectoire.\n",
    "        \"\"\"\n",
    "        Deltas = {}\n",
    "        n = len(self.C)\n",
    "        \n",
    "        # Calcul des projections directes\n",
    "        for i in range(n - 1):\n",
    "            Deltas[(i, i+1)] = self.transformation.project_to_tangent(self.C[i], self.C[i+1])\n",
    "\n",
    "        # Calcul des transports successifs\n",
    "        for i in range(n - 1):\n",
    "            for j in range(i + 2, n):\n",
    "                Deltas[(i, j)] = self.transformation.transport_to_tangent(Deltas[(i, j-1)], self.C[j-1], self.C[j])\n",
    "\n",
    "        return Deltas\n",
    "\n",
    "    def get_delta(self, i, j):\n",
    "        \"\"\"\n",
    "        Obtenir l'incrément ∆i,j\n",
    "        \"\"\"\n",
    "        return self.Deltas.get((i, j), None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AntiDevelopment:\n",
    "    def __init__(self, covariance_matrices):\n",
    "        \"\"\"\n",
    "        Initialiser avec une liste de matrices de covariance.\n",
    "        \"\"\"\n",
    "        self.C = covariance_matrices\n",
    "        self.transformation = Transformation()\n",
    "        self.A = self.calculate_antidevelopment()\n",
    "\n",
    "    def calculate_antidevelopment(self):\n",
    "        \"\"\"\n",
    "        Calculer l'anti-développement de la suite des matrices SPD.\n",
    "        \"\"\"\n",
    "        A = []\n",
    "        n = len(self.C)\n",
    "        \n",
    "        if n > 0:\n",
    "            # Initialiser A1\n",
    "            delta_0_1 = self.transformation.project_to_tangent(self.C[0], self.C[1])\n",
    "            A.append((self.C[0], [delta_0_1]))\n",
    "\n",
    "            for i in range(1, n - 1):\n",
    "                # Calcul de ∆i, i+1\n",
    "                delta_i_i1 = self.transformation.project_to_tangent(self.C[i], self.C[i+1])\n",
    "                \n",
    "                # Transport des ∆ précédents\n",
    "                deltas = []\n",
    "                for j in range(i):\n",
    "                    delta = self.transformation.transport_to_tangent(A[i-1][1][j], self.C[i-1], self.C[i])\n",
    "                    deltas.append(delta)\n",
    "                deltas.append(delta_i_i1)\n",
    "                \n",
    "                A.append((self.C[i], deltas))\n",
    "        \n",
    "        return A\n",
    "\n",
    "    def get_antidevelopment(self, index):\n",
    "        \"\"\"\n",
    "        Obtenir l'anti-développement An pour un index donné.\n",
    "        \"\"\"\n",
    "        if 0 <= index < len(self.A):\n",
    "            return self.A[index]\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TrajectorySPD:\n",
    "    def __init__(self, anti_development, stride, frequency):\n",
    "        \"\"\"\n",
    "        Initialiser avec une liste de couples (Cn, deltas), le stride, et la fréquence d'échantillonnage.\n",
    "        anti_development : liste de tuples (Cn, deltas)\n",
    "        stride : décalage entre les segments\n",
    "        frequency : fréquence d'échantillonnage en Hz\n",
    "        \"\"\"\n",
    "        self.A = anti_development\n",
    "        self.stride = stride\n",
    "        self.frequency = frequency\n",
    "        self.time_stamps = self.calculate_time_stamps()\n",
    "        self.transformation = Transformation()\n",
    "        self.T = self.calculate_trajectory()\n",
    "\n",
    "    def calculate_time_stamps(self):\n",
    "        \"\"\"\n",
    "        Calculer les instants d'échantillonnage.\n",
    "        \"\"\"\n",
    "        n = len(self.A)\n",
    "        t = np.arange(n) * (self.stride / self.frequency)\n",
    "        return t\n",
    "\n",
    "    def calculate_trajectory(self):\n",
    "        \"\"\"\n",
    "        Calculer la trajectoire à partir de l'anti-développement et des instants d'échantillonnage.\n",
    "        \"\"\"\n",
    "        T = {}\n",
    "        k = len(self.A) - 1\n",
    "\n",
    "        for j in range(k, -1, -1):\n",
    "            time_diff = self.time_stamps[j] - self.time_stamps[j-1]\n",
    "            delta_transported = self.transformation.transport_to_tangent(self.A[k][1][j-1], self.A[k][0], np.eye(self.A[k][0].shape[0]))\n",
    "            T[j] = time_diff * delta_transported \n",
    "        \n",
    "        for i in range(k-1, 0, -1):\n",
    "            T[i] = T[i+1] + T[i]\n",
    "\n",
    "        return T\n",
    "\n",
    "    def get_trajectory(self, index):\n",
    "        \"\"\"\n",
    "        Obtenir la trajectoire à l'index donné.\n",
    "        \"\"\"\n",
    "        return self.T.get(index, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    }
   ],
   "source": [
    "dataset = BNCI2014_001()\n",
    "sessions = dataset.get_data(subjects=[1])\n",
    "subject = 1\n",
    "session_name = \"1test\"\n",
    "run_name = \"0\"\n",
    "raw_test = sessions[subject][session_name][run_name]\n",
    "raw_test=raw_test.pick_types(eeg=True,exclude='bads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    X = raw.get_data()\n",
    "    X_test = raw_test.get_data()\n",
    "    f = raw.info['sfreq'] # fréquence d'échantillonnage\n",
    "    l = 500  # longueur du segment en échantillons (par exemple 2 secondes si sfreq=250 Hz)\n",
    "    s = 250  # stride en échantillons (par exemple 1 seconde si sfreq=250 Hz)\n",
    "    segmentation = Segmentation(raw, l, s)\n",
    "    labels = segmentation.segment_labels()\n",
    "    segmentation_test = Segmentation(raw_test, l, s)\n",
    "    labels_test = segmentation_test.segment_labels()\n",
    "\n",
    "    # Créer une instance de la classe et calculer les matrices de covariance\n",
    "    calc = CovarianceCalculator(X, l, s)\n",
    "    calc_test = CovarianceCalculator(X_test, l, s)\n",
    "\n",
    "    \n",
    "    # Créer une instance de Trajectory pour calculer les trajectoires à partir des matrices de covariance\n",
    "    traj = Trajectory(calc.C)\n",
    "    traj_test = Trajectory(calc_test.C)\n",
    "    \n",
    "\n",
    "    # Créer une instance de AntiDevelopment pour calculer l'anti-développement\n",
    "    anti_dev = AntiDevelopment(calc.C)\n",
    "    anti_dev_test = AntiDevelopment(calc_test.C)\n",
    "    \n",
    "    # Créer une instance de TrajectorySPD pour calculer la trajectoire\n",
    "    trajectory_spd = TrajectorySPD(anti_dev.A, s, f)\n",
    "    trajectory_spd_test = TrajectorySPD(anti_dev_test.A, s, f)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.pop(-1)\n",
    "labels_test.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire de mappage des labels\n",
    "label_mapping = {\n",
    "    '': 0,\n",
    "    'tongue': 1,\n",
    "    'feet': 2,\n",
    "    'left_hand': 3,\n",
    "    'right_hand': 4\n",
    "}\n",
    "\n",
    "# Conversion des labels en nombres\n",
    "labels_train = [label_mapping[label[0]] if label else 0 for label in labels]\n",
    "labels_test = [label_mapping[label[0]] if label else 0 for label in labels_test]    \n",
    "\n",
    "# Conversion en array numpy\n",
    "labels_train = np.array(labels_train)\n",
    "labels_test = np.array(labels_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "antidev =[]\n",
    "for i in range(0, len(anti_dev.A)):\n",
    "    antidev.append(anti_dev.A[i][1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "antidev_test = []\n",
    "for i in range(0, len(anti_dev_test.A)):\n",
    "    antidev_test.append(anti_dev_test.A[i][1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectoire = []\n",
    "for i in range(0, len(trajectory_spd.T)):\n",
    "    trajectoire.append(trajectory_spd.T[i]) \n",
    "    trajectoire_test = []\n",
    "for i in range(0, len(trajectory_spd_test.T)):\n",
    "    trajectoire_test.append(trajectory_spd_test.T[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_tain X_test covariance from 2 different sequence          MDM    SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (385, 22, 22)\n",
      "X_test shape: (385, 22, 22)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (384,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 385 but corresponding boolean dimension is 384",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[232], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m clf \u001b[38;5;241m=\u001b[39m make_pipeline( MDM(metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mriemann\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Entraînement du modèle\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Prédiction sur l'ensemble de test\u001b[39;00m\n\u001b[0;32m     32\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params_last_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\pyriemann\\classification.py:121\u001b[0m, in \u001b[0;36mMDM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    118\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovmeans_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    122\u001b[0m         mean_covariance(X[y \u001b[38;5;241m==\u001b[39m ll], metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_mean,\n\u001b[0;32m    123\u001b[0m                         sample_weight\u001b[38;5;241m=\u001b[39msample_weight[y \u001b[38;5;241m==\u001b[39m ll])\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ll \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_]\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovmeans_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m    127\u001b[0m         delayed(mean_covariance)(X[y \u001b[38;5;241m==\u001b[39m ll], metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_mean,\n\u001b[0;32m    128\u001b[0m                                  sample_weight\u001b[38;5;241m=\u001b[39msample_weight[y \u001b[38;5;241m==\u001b[39m ll])\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ll \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\pyriemann\\classification.py:122\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    118\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovmeans_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 122\u001b[0m         mean_covariance(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mll\u001b[49m\u001b[43m]\u001b[49m, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_mean,\n\u001b[0;32m    123\u001b[0m                         sample_weight\u001b[38;5;241m=\u001b[39msample_weight[y \u001b[38;5;241m==\u001b[39m ll])\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ll \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_]\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcovmeans_ \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)(\n\u001b[0;32m    127\u001b[0m         delayed(mean_covariance)(X[y \u001b[38;5;241m==\u001b[39m ll], metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_mean,\n\u001b[0;32m    128\u001b[0m                                  sample_weight\u001b[38;5;241m=\u001b[39msample_weight[y \u001b[38;5;241m==\u001b[39m ll])\n\u001b[0;32m    129\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ll \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 385 but corresponding boolean dimension is 384"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.classification import MDM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# calc_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(calc.C)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(calc_test.C)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "# Création du pipeline avec le classifieur MDM\n",
    "clf = make_pipeline( MDM(metric='riemann'))\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (385, 22, 22)\n",
      "X_test shape: (385, 22, 22)\n",
      "y_train shape: (385,)\n",
      "y_test shape: (385,)\n",
      "Accuracy: 24.94%\n",
      "Labels prédits :  [3 3 2 1 0 0 0 1 4 0 0 3 3 3 0 0 1 3 2 3 0 0 3 3 0 0 0 1 3 3 0 0 1 3 3 0 0\n",
      " 0 3 3 3 0 0 1 3 0 3 1 1 3 0 3 0 3 1 1 0 0 1 1 0 1 1 1 3 0 0 2 0 3 3 0 3 3\n",
      " 3 3 0 1 3 3 3 3 3 3 3 0 3 0 3 3 1 1 0 0 3 3 3 1 3 3 3 3 3 0 1 3 3 1 0 3 0\n",
      " 1 3 3 3 3 3 0 0 0 3 1 1 3 3 2 3 2 2 3 3 1 0 0 0 2 3 0 3 3 1 1 3 0 0 0 4 0\n",
      " 0 3 3 3 1 0 1 0 0 3 3 3 0 0 0 0 3 0 0 0 3 2 3 1 3 4 3 3 3 0 0 1 3 4 4 3 0\n",
      " 0 1 3 3 3 3 3 3 0 0 1 1 0 3 3 0 1 1 3 3 1 2 2 3 3 3 3 3 3 0 0 1 0 3 3 3 3\n",
      " 0 0 3 3 0 0 2 2 1 1 3 3 3 3 0 2 0 0 0 3 3 3 3 0 0 0 0 3 4 4 4 1 3 3 3 0 3\n",
      " 0 1 0 3 3 1 1 3 3 0 4 3 3 1 1 1 2 4 3 3 0 0 1 0 0 0 3 3 1 3 0 1 1 3 3 4 0\n",
      " 3 3 3 1 3 3 0 0 1 2 1 1 1 3 3 3 1 1 1 1 0 0 3 3 3 3 0 0 3 3 3 3 1 1 3 3 1\n",
      " 1 1 0 0 2 3 3 0 0 2 3 3 4 3 1 3 3 3 2 2 0 3 1 2 1 0 0 0 3 3 3 3 1 3 3 0 2\n",
      " 0 3 3 3 3 3 3 3 1 3 0 2 2 0 3]\n",
      "Labels réels :  [3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# calc_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(calc.C)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(calc_test.C)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "\n",
    "# Création du pipeline avec la transformation en espace tangent et le classifieur SVM\n",
    "clf = make_pipeline(TangentSpace(metric='riemann'), SVC(kernel='linear'))\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train X_test Covariance from 1 sequence          SVM     CSP+LDA     MDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (269, 22, 22)\n",
      "X_test shape: (116, 22, 22)\n",
      "y_train shape: (269,)\n",
      "y_test shape: (115,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [115, 116]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Évaluation du modèle\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Affichage des résultats\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    212\u001b[0m         )\n\u001b[0;32m    213\u001b[0m     ):\n\u001b[1;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    224\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:220\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[0;32m    156\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ABC\\anaconda3\\envs\\py38\\lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [115, 116]"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "\n",
    "# Conversion des listes en arrays numpy\n",
    "X = np.array(calc.C)\n",
    "y = np.array(labels_train)\n",
    "\n",
    "# Calcul de la taille de l'ensemble d'entraînement\n",
    "n_train = int(0.7 * len(X))\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test (70% entraînement, 30% test)\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "\n",
    "# Vérification des dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples_train, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples_test, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples_train,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples_test,)\n",
    "\n",
    "# Création du pipeline avec la transformation en espace tangent et le classifieur SVM\n",
    "clf = make_pipeline(TangentSpace(metric='riemann'), SVC(kernel='linear'))\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (269, 22, 22)\n",
      "X_test shape: (116, 22, 22)\n",
      "y_train shape: (269,)\n",
      "y_test shape: (116,)\n",
      "Accuracy: 28.45%\n",
      "Labels prédits :  [1 0 0 0 1 1 4 4 4 4 0 0 0 4 4 4 2 2 2 4 0 0 3 3 2 2 2 1 1 1 1 0 4 3 4 0 0\n",
      " 1 3 4 4 3 0 0 1 1 0 0 0 2 1 1 0 1 2 4 4 4 4 2 1 1 1 2 4 4 1 1 1 1 1 0 4 4\n",
      " 0 0 1 1 1 4 4 4 2 0 0 0 4 2 2 2 4 0 0 2 3 3 3 3 3 0 0 2 2 2 3 4 4 0 0 3 0\n",
      " 4 3 4 0 0]\n",
      "Labels réels :  [2 2 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 2 2 2 2 2 2 0 0 3\n",
      " 3 3 3 3 3 0 0 1 1 1 1 1 1 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4\n",
      " 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 0 3 3\n",
      " 3 3 3 3 0]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.classification import MDM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "\n",
    "# Conversion des listes en arrays numpy\n",
    "X = np.array(calc.C)\n",
    "y = np.array(labels_train)\n",
    "\n",
    "# Calcul de la taille de l'ensemble d'entraînement\n",
    "n_train = int(0.7 * len(X))\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test (70% entraînement, 30% test)\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "\n",
    "# Vérification des dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples_train, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples_test, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples_train,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples_test,)\n",
    "\n",
    "# Création du pipeline avec la transformation en espace tangent et le classifieur SVM\n",
    "clf = make_pipeline(MDM(metric='riemann'))\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "\n",
    "# Conversion des listes en arrays numpy\n",
    "X = np.array(calc.C)\n",
    "y = np.array(labels_train)\n",
    "\n",
    "# Calcul de la taille de l'ensemble d'entraînement\n",
    "n_train = int(0.7 * len(X))\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test (70% entraînement, 30% test)\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_test = X[n_train:]\n",
    "y_test = y[n_train:]\n",
    "\n",
    "# Vérification des dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples_train, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples_test, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples_train,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples_test,)\n",
    "\n",
    "# Création du pipeline avec la transformation en espace tangent et le classifieur SVM\n",
    "clf = make_pipeline(CSP(n_components=4, reg=None, norm_trace=False), LDA())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Using Antidevelopment       CSP + LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (384, 22, 22)\n",
      "X_test shape: (384, 22, 22)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (384,)\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.066 (2.2e-16 eps * 22 dim * 1.3e+13  max singular value)\n",
      "    Estimated rank (mag): 22\n",
      "    MAG: rank 22 computed from 22 data channels with 0 projectors\n",
      "Reducing data rank from 22 -> 22\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.052 (2.2e-16 eps * 22 dim * 1.1e+13  max singular value)\n",
      "    Estimated rank (mag): 22\n",
      "    MAG: rank 22 computed from 22 data channels with 0 projectors\n",
      "Reducing data rank from 22 -> 22\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.046 (2.2e-16 eps * 22 dim * 9.5e+12  max singular value)\n",
      "    Estimated rank (mag): 22\n",
      "    MAG: rank 22 computed from 22 data channels with 0 projectors\n",
      "Reducing data rank from 22 -> 22\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.048 (2.2e-16 eps * 22 dim * 9.9e+12  max singular value)\n",
      "    Estimated rank (mag): 22\n",
      "    MAG: rank 22 computed from 22 data channels with 0 projectors\n",
      "Reducing data rank from 22 -> 22\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.046 (2.2e-16 eps * 22 dim * 9.5e+12  max singular value)\n",
      "    Estimated rank (mag): 22\n",
      "    MAG: rank 22 computed from 22 data channels with 0 projectors\n",
      "Reducing data rank from 22 -> 22\n",
      "Estimating covariance using EMPIRICAL\n",
      "Done.\n",
      "Accuracy: 25.52%\n",
      "Labels prédits :  [2 0 0 2 3 2 4 1 4 4 2 3 2 2 0 1 3 2 0 0 0 0 3 3 2 3 3 2 3 1 0 3 2 3 0 4 0\n",
      " 3 3 3 1 2 3 3 3 0 0 0 3 0 0 0 2 3 0 3 0 0 3 3 0 0 0 2 0 2 0 1 0 0 3 0 0 0\n",
      " 0 3 2 1 2 1 1 4 1 3 0 0 0 0 1 0 2 2 0 3 1 0 1 3 2 3 2 3 4 2 3 3 1 4 3 4 3\n",
      " 0 2 3 0 3 2 0 0 2 0 1 3 3 2 2 4 2 0 3 3 0 2 2 3 0 2 3 3 2 1 0 3 2 1 0 0 4\n",
      " 1 0 3 3 0 3 3 3 3 3 3 2 2 0 2 1 2 0 2 0 3 2 2 1 2 3 0 3 0 0 4 1 3 3 0 0 3\n",
      " 0 0 3 3 2 3 3 2 3 4 3 4 2 0 3 0 3 3 3 4 3 2 0 2 0 3 3 4 0 2 0 0 0 3 3 3 0\n",
      " 0 4 4 0 2 0 2 0 0 0 0 0 3 2 3 2 3 0 3 0 3 3 3 0 3 3 0 4 2 0 3 2 4 0 1 4 0\n",
      " 1 3 3 2 3 0 3 0 2 2 4 0 3 3 4 0 1 4 3 2 3 2 0 2 0 2 3 3 4 2 3 3 2 3 3 2 3\n",
      " 2 2 1 2 2 3 3 0 2 0 0 2 0 3 3 0 2 0 3 2 2 2 2 3 0 3 3 0 2 3 2 2 3 2 0 0 3\n",
      " 0 3 3 3 2 3 2 0 2 3 3 3 0 3 0 1 0 3 0 3 0 0 1 2 2 3 2 2 3 0 3 0 3 2 0 2 0\n",
      " 2 3 3 3 3 2 0 3 2 2 2 3 0 0]\n",
      "Labels réels :  [3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.svm import SVC\n",
    "from pyriemann.classification import MDM\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# calc_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(antidev)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(antidev_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "\n",
    "\n",
    "# Création du pipeline avec la transformation en espace tangent et le classifieur CSP LDA\n",
    "clf = make_pipeline(CSP(n_components=4, reg=None, norm_trace=False), LDA())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.svm import SVC\n",
    "from pyriemann.classification import MDM\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# calc_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(trajectoire)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(trajectoire_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "\n",
    "# Création du pipeline avec la transformation en espace tangent et le classifieur SVM\n",
    "clf = make_pipeline(CSP(n_components=2, reg='ledoit_wolf', norm_trace=False), LDA())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using antidevelopment and flatte the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (384, 22, 22)\n",
      "X_test shape: (384, 22, 22)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (384,)\n",
      "X_train_flattened shape: (384, 484)\n",
      "X_test_flattened shape: (384, 484)\n",
      "Accuracy: 27.08%\n",
      "Labels prédits :  [3 1 1 2 0 3 0 0 1 3 2 2 4 3 0 4 1 3 0 0 3 4 1 0 3 4 4 2 0 3 1 0 2 0 4 2 0\n",
      " 3 1 1 2 4 0 2 1 0 3 4 4 4 2 0 3 3 0 2 1 0 3 3 0 0 0 2 0 4 3 3 3 1 0 2 1 3\n",
      " 1 4 2 4 0 1 1 3 0 1 3 0 4 1 0 3 1 2 2 0 0 4 3 0 2 2 4 0 1 3 0 0 2 3 3 0 0\n",
      " 4 3 1 2 4 2 0 0 1 3 4 1 0 4 0 1 1 2 3 0 3 1 0 3 1 3 2 0 1 1 0 2 4 3 0 3 2\n",
      " 4 0 2 3 1 0 4 1 3 0 1 0 3 3 3 2 1 4 0 0 0 2 0 3 0 1 0 0 2 1 0 1 0 2 2 1 0\n",
      " 0 1 1 3 0 0 3 1 1 3 4 2 3 4 2 0 0 4 0 1 1 2 0 3 3 4 0 0 2 1 3 4 1 3 2 0 0\n",
      " 0 4 1 1 1 2 4 0 0 1 4 1 0 1 1 4 0 0 4 4 2 1 2 1 0 3 3 4 4 3 2 3 1 2 4 0 1\n",
      " 0 4 0 2 4 0 0 3 4 2 3 4 4 1 3 4 2 0 0 1 2 0 0 4 0 1 0 4 1 0 0 4 0 4 3 0 1\n",
      " 4 0 0 0 2 3 4 0 1 4 0 1 3 0 1 0 3 2 3 4 2 3 4 4 0 0 2 2 4 2 0 3 0 1 0 1 1\n",
      " 2 1 3 0 4 1 2 3 4 3 0 2 2 0 1 4 3 3 0 0 4 2 0 3 4 0 0 1 0 4 4 1 1 2 3 3 0\n",
      " 4 3 0 4 0 0 3 1 1 4 0 4 1 2]\n",
      "Labels réels :  [3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# antidev est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_samples, n_channels, n_channels))\n",
    "# labels_train est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# antidev_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(antidev)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(antidev_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "# Aplatissement des matrices de covariance en vecteurs\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Vérifiez les dimensions après aplatissement\n",
    "print(\"X_train_flattened shape:\", X_train_flattened.shape)  # Doit être (n_samples, n_channels * n_channels)\n",
    "print(\"X_test_flattened shape:\", X_test_flattened.shape)    # Doit être (n_samples, n_channels * n_channels)\n",
    "\n",
    "# Création du pipeline avec CSP et le classifieur LDA\n",
    "clf = make_pipeline( LDA())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train_flattened, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test_flattened)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (384, 22, 22)\n",
      "X_test shape: (384, 22, 22)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (384,)\n",
      "X_train_flattened shape: (384, 484)\n",
      "X_test_flattened shape: (384, 484)\n",
      "Accuracy: 27.08%\n",
      "Labels prédits :  [3 1 1 2 0 3 0 0 1 3 2 2 4 3 0 4 1 3 0 0 3 4 1 0 3 4 4 2 0 3 1 0 2 0 4 2 0\n",
      " 3 1 1 2 4 0 2 1 0 3 4 4 4 2 0 3 3 0 2 1 0 3 3 0 0 0 2 0 4 3 3 3 1 0 2 1 3\n",
      " 1 4 2 4 0 1 1 3 0 1 3 0 4 1 0 3 1 2 2 0 0 4 3 0 2 2 4 0 1 3 0 0 2 3 3 0 0\n",
      " 4 3 1 2 4 2 0 0 1 3 4 1 0 4 0 1 1 2 3 0 3 1 0 3 1 3 2 0 1 1 0 2 4 3 0 3 2\n",
      " 4 0 2 3 1 0 4 1 3 0 1 0 3 3 3 2 1 4 0 0 0 2 0 3 0 1 0 0 2 1 0 1 0 2 2 1 0\n",
      " 0 1 1 3 0 0 3 1 1 3 4 2 3 4 2 0 0 4 0 1 1 2 0 3 3 4 0 0 2 1 3 4 1 3 2 0 0\n",
      " 0 4 1 1 1 2 4 0 0 1 4 1 0 1 1 4 0 0 4 4 2 1 2 1 0 3 3 4 4 3 2 3 1 2 4 0 1\n",
      " 0 4 0 2 4 0 0 3 4 2 3 4 4 1 3 4 2 0 0 1 2 0 0 4 0 1 0 4 1 0 0 4 0 4 3 0 1\n",
      " 4 0 0 0 2 3 4 0 1 4 0 1 3 0 1 0 3 2 3 4 2 3 4 4 0 0 2 2 4 2 0 3 0 1 0 1 1\n",
      " 2 1 3 0 4 1 2 3 4 3 0 2 2 0 1 4 3 3 0 0 4 2 0 3 4 0 0 1 0 4 4 1 1 2 3 3 0\n",
      " 4 3 0 4 0 0 3 1 1 4 0 4 1 2]\n",
      "Labels réels :  [3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# antidev est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_samples, n_channels, n_channels))\n",
    "# labels_train est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# antidev_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(antidev)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(antidev_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "# Aplatissement des matrices de covariance en vecteurs en utilisant .flatten()\n",
    "X_train_flattened = np.array([mat.flatten() for mat in X_train])\n",
    "X_test_flattened = np.array([mat.flatten() for mat in X_test])\n",
    "\n",
    "# Vérifiez les dimensions après aplatissement\n",
    "print(\"X_train_flattened shape:\", X_train_flattened.shape)  # Doit être (n_samples, n_channels * n_channels)\n",
    "print(\"X_test_flattened shape:\", X_test_flattened.shape)    # Doit être (n_samples, n_channels * n_channels)\n",
    "\n",
    "# Création du pipeline avec le classifieur LDA\n",
    "clf = make_pipeline(LDA())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train_flattened, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test_flattened)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (384, 22, 22)\n",
      "X_test shape: (384, 22, 22)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (384,)\n",
      "X_train_flattened shape: (384, 484)\n",
      "X_test_flattened shape: (384, 484)\n",
      "Accuracy: 29.43%\n",
      "Labels prédits :  [1 2 1 0 0 2 2 0 2 1 2 2 0 0 0 0 1 3 0 0 2 1 0 0 0 1 0 2 3 0 0 0 2 0 4 0 2\n",
      " 0 0 0 1 1 0 2 2 0 0 0 2 3 3 0 0 0 0 2 0 1 2 0 0 0 0 2 1 3 2 0 0 0 0 2 1 1\n",
      " 0 2 2 0 0 1 0 2 0 0 0 0 3 3 1 0 1 2 0 0 4 2 2 0 0 0 0 0 0 0 0 2 2 2 0 0 1\n",
      " 1 0 2 3 0 2 0 0 2 1 0 2 0 2 0 0 1 2 2 0 2 2 0 0 0 0 2 3 0 3 0 2 0 0 1 2 2\n",
      " 0 0 0 2 2 0 1 2 2 2 0 0 2 0 0 1 0 0 2 1 0 2 2 2 0 0 0 0 2 1 3 4 2 0 0 0 3\n",
      " 1 0 2 0 2 0 0 0 2 0 0 2 0 0 2 0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 3 4 1 0 2 2 0\n",
      " 0 2 1 0 2 2 0 0 0 0 0 1 0 2 2 2 0 0 0 4 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 2 3\n",
      " 0 2 0 0 2 0 0 2 3 2 0 0 0 0 1 2 2 0 0 0 0 0 0 2 1 4 0 2 0 0 0 0 2 2 0 0 2\n",
      " 0 0 0 0 2 0 2 0 2 0 0 2 3 0 0 0 0 4 1 0 2 0 0 2 0 0 2 1 0 2 2 2 0 0 2 0 0\n",
      " 2 2 0 0 0 2 2 0 0 4 0 0 0 2 1 0 0 3 0 0 4 0 0 2 2 0 0 2 0 0 2 0 2 0 0 0 0\n",
      " 2 2 2 0 0 0 0 0 2 0 0 3 0 0]\n",
      "Labels réels :  [3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# antidev est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_samples, n_channels, n_channels))\n",
    "# labels_train est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# antidev_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(antidev)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(antidev_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "# Aplatissement des matrices de covariance en vecteurs en utilisant .flatten()\n",
    "X_train_flattened = np.array([mat.flatten() for mat in X_train])\n",
    "X_test_flattened = np.array([mat.flatten() for mat in X_test])\n",
    "\n",
    "# Vérifiez les dimensions après aplatissement\n",
    "print(\"X_train_flattened shape:\", X_train_flattened.shape)  # Doit être (n_samples, n_channels * n_channels)\n",
    "print(\"X_test_flattened shape:\", X_test_flattened.shape)    # Doit être (n_samples, n_channels * n_channels)\n",
    "\n",
    "# Création du pipeline avec le classifieur LDA\n",
    "clf = make_pipeline(SVC())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train_flattened, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test_flattened)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use trajectoire with flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (384, 22, 22)\n",
      "X_test shape: (384, 22, 22)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (384,)\n",
      "X_train_flattened shape: (384, 484)\n",
      "X_test_flattened shape: (384, 484)\n",
      "Accuracy: 24.74%\n",
      "Labels prédits :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Labels réels :  [3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# antidev est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_samples, n_channels, n_channels))\n",
    "# labels_train est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# antidev_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(trajectoire)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(trajectoire_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "# Aplatissement des matrices de covariance en vecteurs en utilisant .flatten()\n",
    "X_train_flattened = np.array([mat.flatten() for mat in X_train])\n",
    "X_test_flattened = np.array([mat.flatten() for mat in X_test])\n",
    "\n",
    "# Vérifiez les dimensions après aplatissement\n",
    "print(\"X_train_flattened shape:\", X_train_flattened.shape)  # Doit être (n_samples, n_channels * n_channels)\n",
    "print(\"X_test_flattened shape:\", X_test_flattened.shape)    # Doit être (n_samples, n_channels * n_channels)\n",
    "\n",
    "# Création du pipeline avec le classifieur LDA\n",
    "clf = make_pipeline(SVC())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train_flattened, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test_flattened)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (384, 22, 22)\n",
      "X_test shape: (384, 22, 22)\n",
      "y_train shape: (384,)\n",
      "y_test shape: (384,)\n",
      "X_train_flattened shape: (384, 484)\n",
      "X_test_flattened shape: (384, 484)\n",
      "Accuracy: 24.74%\n",
      "Labels prédits :  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Labels réels :  [3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# antidev est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_samples, n_channels, n_channels))\n",
    "# labels_train est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# antidev_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(trajectoire)\n",
    "y_train = np.array(labels_train)\n",
    "X_test = np.array(trajectoire_test)\n",
    "y_test = np.array(labels_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "# Aplatissement des matrices de covariance en vecteurs en utilisant .flatten()\n",
    "X_train_flattened = np.array([mat.flatten() for mat in X_train])\n",
    "X_test_flattened = np.array([mat.flatten() for mat in X_test])\n",
    "\n",
    "# Vérifiez les dimensions après aplatissement\n",
    "print(\"X_train_flattened shape:\", X_train_flattened.shape)  # Doit être (n_samples, n_channels * n_channels)\n",
    "print(\"X_test_flattened shape:\", X_test_flattened.shape)    # Doit être (n_samples, n_channels * n_channels)\n",
    "\n",
    "# Création du pipeline avec le classifieur LDA\n",
    "clf = make_pipeline(LDA())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train_flattened, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test_flattened)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats = []\n",
    "dernier_label_non_vide = None\n",
    "label_non_vide = None\n",
    "label_deja_ajoute = False\n",
    "\n",
    "for i in range(len(labels_train)):\n",
    "    if labels_train[i] == 0:\n",
    "        if not label_deja_ajoute:\n",
    "            resultats.append((dernier_label_non_vide, label_non_vide))\n",
    "            label_deja_ajoute = True\n",
    "    elif labels_train[i] != \"\":\n",
    "        dernier_label_non_vide = trajectoire[i]  \n",
    "        label_non_vide = labels_train[i]\n",
    "        label_deja_ajoute = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultats_test = []\n",
    "dernier_label_non_vide = None\n",
    "label_non_vide = None\n",
    "label_deja_ajoute = False\n",
    "\n",
    "for i in range(len(labels_test)):\n",
    "    if labels_test[i] == 0:\n",
    "        if not label_deja_ajoute:\n",
    "            resultats_test.append((dernier_label_non_vide, label_non_vide))\n",
    "            label_deja_ajoute = True\n",
    "    elif labels_test[i] != \"\":\n",
    "        dernier_label_non_vide = trajectoire_test[i]  \n",
    "        label_non_vide = labels_test[i]\n",
    "        label_deja_ajoute = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_label = []\n",
    "last_label_trajectoire =[]\n",
    "for i in range(0, len(resultats)):\n",
    "    last_label_trajectoire.append(resultats[i][0])\n",
    "    last_label.append(resultats[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_label_test = []\n",
    "last_label_trajectoire_test =[]\n",
    "for i in range(0, len(resultats_test)):\n",
    "    last_label_trajectoire_test.append(resultats_test[i][0])\n",
    "    last_label_test.append(resultats_test[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 0 0 2 2 2 2 2 2 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 3 3 3 3 3 3\n",
      " 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 4 4 4 4 4 4 0 0 2 2 2\n",
      " 2 2 2 0 0 3 3 3 3 3 3 0 3 3 3 3 3 3 0 0 3 3 3 3 3 3 0 0 1 1 1 1 1 1 0 0 4\n",
      " 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2\n",
      " 2 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 0 0 2 2\n",
      " 2 2 2 2 0 0 3 3 3 3 3 3 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 4 4 4 4 4 4 0\n",
      " 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 0 4 4\n",
      " 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0\n",
      " 0 2 2 2 2 2 2 0 0 3 3 3 3 3 3 0 0 1 1 1 1 1 1 0 0 4 4 4 4 4 4 0 0 2 2 2 2\n",
      " 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 4 4 4 4 4 4 0 0 2\n",
      " 2 2 2 2 2 0 0 0 3 3 3 3 3 3]\n",
      "[3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4\n",
      " 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 1 1 1\n",
      " 1 1 1 0 0 3 3 3 3 3 3 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 0 0 1 1 1 1 1\n",
      " 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0 0 0 4 4 4 4 4 4 0 3 3 3 3 3 3 0 0 3 3\n",
      " 3 3 3 3 0 0 2 2 2 2 2 2 0 0 1 1 1 1 1 1 0 0 3 3 3 3 3 3 0 0 2 2 2 2 2 2 0\n",
      " 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0 0 0 3 3\n",
      " 3 3 3 3 0 0 4 4 4 4 4 4 0 0 4 4 4 4 4 4 0 0 3 3 3 3 3 3 0 0 4 4 4 4 4 4 0\n",
      " 0 2 2 2 2 2 2 0 0 4 4 4 4 4 4 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 0 0 1 1 1 1\n",
      " 1 1 0 0 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 0 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 1 1 0 0 0 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels_train)\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (47, 22, 22)\n",
      "X_test shape: (47, 22, 22)\n",
      "y_train shape: (47,)\n",
      "y_test shape: (47,)\n",
      "X_train_flattened shape: (47, 484)\n",
      "X_test_flattened shape: (47, 484)\n",
      "Accuracy: 25.53%\n",
      "Labels prédits :  [4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4 4 4 4 4 4 4]\n",
      "Labels réels :  [3 4 4 3 4 3 4 2 4 1 3 2 4 3 1 1 1 1 1 3 2 4 3 3 2 1 3 2 2 2 3 4 3 4 4 3 4\n",
      " 2 4 2 2 1 2 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# antidev est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_samples, n_channels, n_channels))\n",
    "# labels_train est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# antidev_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(last_label_trajectoire)\n",
    "y_train = np.array(last_label)\n",
    "X_test = np.array(last_label_trajectoire_test)\n",
    "y_test = np.array(last_label_test)\n",
    "\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "# Flattening the matrices into vectors\n",
    "X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test_flattened = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "# Vérifiez les dimensions après aplatissement\n",
    "print(\"X_train_flattened shape:\", X_train_flattened.shape)  # Doit être (n_samples, n_channels * n_channels)\n",
    "print(\"X_test_flattened shape:\", X_test_flattened.shape)    # Doit être (n_samples, n_channels * n_channels)\n",
    "\n",
    "# Création du pipeline avec le classifieur LDA\n",
    "clf = make_pipeline(SVC())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train_flattened, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test_flattened)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import numpy as np\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Supposons que vous avez vos données\n",
    "# x: calc.C est une liste de matrices de covariance (par exemple, une liste de np.ndarray de forme (n_channels, n_channels))\n",
    "# y: labels est une liste de labels correspondants (par exemple, une liste d'entiers ou de chaînes de caractères)\n",
    "# calc_test et labels_test sont vos ensembles de test\n",
    "\n",
    "# Conversion des listes en arrays numpy et vérification des dimensions\n",
    "X_train = np.array(last_label_trajectoire)\n",
    "y_train = np.array(last_label)\n",
    "X_test = np.array(last_label_trajectoire_test)\n",
    "y_test = np.array(last_label_test)\n",
    "\n",
    "# Vérifiez les dimensions des matrices de covariance\n",
    "print(\"X_train shape:\", X_train.shape)  # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"X_test shape:\", X_test.shape)    # Doit être (n_samples, n_channels, n_channels)\n",
    "print(\"y_train shape:\", y_train.shape)  # Doit être (n_samples,)\n",
    "print(\"y_test shape:\", y_test.shape)    # Doit être (n_samples,)\n",
    "\n",
    "\n",
    "# Création du pipeline avec la transformation en espace tangent et le classifieur SVM\n",
    "clf = make_pipeline(CSP(n_components=1, reg='ledoit_wolf', norm_trace=False), LDA())\n",
    "\n",
    "# Entraînement du modèle\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Évaluation du modèle\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"Labels prédits : \", y_pred)\n",
    "print(\"Labels réels : \", y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
